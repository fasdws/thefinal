{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bffc88",
   "metadata": {},
   "source": [
    "Import และ config path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3aae4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92859e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting keras\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (2.2.5)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (4.12.0.88)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (80.3.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (4.13.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.71.0)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pillow (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading pillow-12.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.20.0->tensorflow) (2.0.1)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.18.0-cp312-cp312-win_amd64.whl.metadata (35 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.0-cp312-cp312-win_amd64.whl.metadata (115 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\this pc\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "   ---------------------------------------- 0.0/331.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/331.9 MB 9.3 MB/s eta 0:00:36\n",
      "   ---------------------------------------- 3.1/331.9 MB 8.0 MB/s eta 0:00:42\n",
      "    --------------------------------------- 4.7/331.9 MB 7.9 MB/s eta 0:00:42\n",
      "    --------------------------------------- 5.5/331.9 MB 7.5 MB/s eta 0:00:44\n",
      "    --------------------------------------- 7.1/331.9 MB 7.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 8.9/331.9 MB 7.5 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 10.5/331.9 MB 7.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 11.8/331.9 MB 7.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 13.6/331.9 MB 7.5 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 14.9/331.9 MB 7.4 MB/s eta 0:00:43\n",
      "   -- ------------------------------------- 16.8/331.9 MB 7.4 MB/s eta 0:00:43\n",
      "   -- ------------------------------------- 17.6/331.9 MB 7.2 MB/s eta 0:00:44\n",
      "   -- ------------------------------------- 18.4/331.9 MB 6.9 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 19.1/331.9 MB 6.7 MB/s eta 0:00:47\n",
      "   -- ------------------------------------- 19.9/331.9 MB 6.4 MB/s eta 0:00:49\n",
      "   -- ------------------------------------- 20.4/331.9 MB 6.2 MB/s eta 0:00:51\n",
      "   -- ------------------------------------- 21.2/331.9 MB 6.0 MB/s eta 0:00:52\n",
      "   -- ------------------------------------- 22.3/331.9 MB 6.1 MB/s eta 0:00:51\n",
      "   -- ------------------------------------- 23.3/331.9 MB 5.9 MB/s eta 0:00:53\n",
      "   -- ------------------------------------- 24.6/331.9 MB 6.0 MB/s eta 0:00:52\n",
      "   --- ------------------------------------ 26.2/331.9 MB 6.0 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 27.3/331.9 MB 6.0 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 28.8/331.9 MB 6.1 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 30.9/331.9 MB 6.2 MB/s eta 0:00:49\n",
      "   --- ------------------------------------ 32.0/331.9 MB 6.2 MB/s eta 0:00:49\n",
      "   --- ------------------------------------ 33.0/331.9 MB 6.2 MB/s eta 0:00:49\n",
      "   ---- ----------------------------------- 33.8/331.9 MB 6.1 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 34.6/331.9 MB 5.9 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 35.1/331.9 MB 5.8 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 35.4/331.9 MB 5.7 MB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 36.2/331.9 MB 5.6 MB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 37.2/331.9 MB 5.6 MB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 38.3/331.9 MB 5.6 MB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 39.6/331.9 MB 5.6 MB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 40.1/331.9 MB 5.5 MB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 40.6/331.9 MB 5.4 MB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 41.2/331.9 MB 5.4 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 41.9/331.9 MB 5.3 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 43.0/331.9 MB 5.3 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 43.8/331.9 MB 5.3 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 44.8/331.9 MB 5.3 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 45.9/331.9 MB 5.3 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 47.2/331.9 MB 5.3 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 48.0/331.9 MB 5.2 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 49.0/331.9 MB 5.2 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 50.1/331.9 MB 5.2 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 51.4/331.9 MB 5.2 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 52.2/331.9 MB 5.2 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 53.2/331.9 MB 5.2 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 54.5/331.9 MB 5.2 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 55.6/331.9 MB 5.2 MB/s eta 0:00:53\n",
      "   ------ --------------------------------- 56.9/331.9 MB 5.2 MB/s eta 0:00:53\n",
      "   ------ --------------------------------- 57.9/331.9 MB 5.2 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 59.0/331.9 MB 5.2 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 60.0/331.9 MB 5.2 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 60.6/331.9 MB 5.2 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 61.3/331.9 MB 5.2 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 61.9/331.9 MB 5.1 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 62.7/331.9 MB 5.1 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 63.7/331.9 MB 5.1 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 64.5/331.9 MB 5.1 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 65.5/331.9 MB 5.1 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 66.6/331.9 MB 5.1 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 67.4/331.9 MB 5.0 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 68.4/331.9 MB 5.0 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 68.9/331.9 MB 5.0 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 70.3/331.9 MB 5.0 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 70.8/331.9 MB 5.0 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 71.6/331.9 MB 5.0 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 72.9/331.9 MB 5.0 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 73.9/331.9 MB 5.0 MB/s eta 0:00:52\n",
      "   --------- ------------------------------ 75.2/331.9 MB 5.0 MB/s eta 0:00:52\n",
      "   --------- ------------------------------ 76.5/331.9 MB 5.0 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 77.1/331.9 MB 5.0 MB/s eta 0:00:52\n",
      "   --------- ------------------------------ 78.6/331.9 MB 5.0 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 79.4/331.9 MB 5.0 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 80.7/331.9 MB 5.0 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 81.8/331.9 MB 5.0 MB/s eta 0:00:50\n",
      "   --------- ------------------------------ 82.6/331.9 MB 5.0 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 83.9/331.9 MB 5.0 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 84.9/331.9 MB 5.0 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 86.0/331.9 MB 5.0 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 87.0/331.9 MB 5.0 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 88.1/331.9 MB 5.0 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 89.1/331.9 MB 5.0 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 90.2/331.9 MB 5.0 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 91.0/331.9 MB 5.0 MB/s eta 0:00:49\n",
      "   ----------- ---------------------------- 92.0/331.9 MB 5.0 MB/s eta 0:00:49\n",
      "   ----------- ---------------------------- 93.3/331.9 MB 5.0 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 94.1/331.9 MB 5.0 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 95.4/331.9 MB 5.0 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 96.5/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 97.8/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 98.6/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 99.1/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 100.4/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 100.9/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 102.0/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 103.0/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 103.8/331.9 MB 5.0 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 104.9/331.9 MB 5.0 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 105.9/331.9 MB 5.0 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 106.7/331.9 MB 5.0 MB/s eta 0:00:46\n",
      "   ------------- -------------------------- 108.0/331.9 MB 5.0 MB/s eta 0:00:46\n",
      "   ------------- -------------------------- 109.3/331.9 MB 5.0 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 110.1/331.9 MB 5.0 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 111.1/331.9 MB 5.0 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 112.2/331.9 MB 5.0 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 113.2/331.9 MB 5.0 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 114.0/331.9 MB 4.9 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 115.1/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 115.3/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 116.1/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 117.2/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 118.0/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 118.2/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 119.5/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 120.3/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 121.4/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 122.4/331.9 MB 4.9 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 123.5/331.9 MB 4.9 MB/s eta 0:00:43\n",
      "   --------------- ------------------------ 124.8/331.9 MB 4.9 MB/s eta 0:00:43\n",
      "   --------------- ------------------------ 125.6/331.9 MB 4.9 MB/s eta 0:00:43\n",
      "   --------------- ------------------------ 126.9/331.9 MB 4.9 MB/s eta 0:00:43\n",
      "   --------------- ------------------------ 127.9/331.9 MB 4.9 MB/s eta 0:00:42\n",
      "   --------------- ------------------------ 129.0/331.9 MB 4.9 MB/s eta 0:00:42\n",
      "   --------------- ------------------------ 130.5/331.9 MB 4.9 MB/s eta 0:00:42\n",
      "   --------------- ------------------------ 131.6/331.9 MB 4.9 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 132.6/331.9 MB 4.9 MB/s eta 0:00:41\n",
      "   ---------------- ----------------------- 134.0/331.9 MB 4.9 MB/s eta 0:00:41\n",
      "   ---------------- ----------------------- 135.0/331.9 MB 4.9 MB/s eta 0:00:41\n",
      "   ---------------- ----------------------- 136.3/331.9 MB 4.9 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 137.1/331.9 MB 4.9 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 138.4/331.9 MB 4.9 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 139.7/331.9 MB 4.9 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 140.8/331.9 MB 4.9 MB/s eta 0:00:39\n",
      "   ----------------- ---------------------- 141.8/331.9 MB 4.9 MB/s eta 0:00:39\n",
      "   ----------------- ---------------------- 143.1/331.9 MB 4.9 MB/s eta 0:00:39\n",
      "   ----------------- ---------------------- 143.9/331.9 MB 4.9 MB/s eta 0:00:39\n",
      "   ----------------- ---------------------- 145.2/331.9 MB 4.9 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 146.3/331.9 MB 4.9 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 147.3/331.9 MB 4.9 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 148.1/331.9 MB 4.9 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 149.2/331.9 MB 4.9 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 149.9/331.9 MB 4.9 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 151.0/331.9 MB 4.9 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 152.0/331.9 MB 4.9 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 152.8/331.9 MB 4.8 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 154.1/331.9 MB 4.8 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 154.9/331.9 MB 4.8 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 156.2/331.9 MB 4.8 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 157.3/331.9 MB 4.8 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 158.6/331.9 MB 4.8 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 159.9/331.9 MB 4.8 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 161.0/331.9 MB 4.8 MB/s eta 0:00:36\n",
      "   ------------------- -------------------- 162.0/331.9 MB 4.8 MB/s eta 0:00:36\n",
      "   ------------------- -------------------- 163.3/331.9 MB 4.8 MB/s eta 0:00:36\n",
      "   ------------------- -------------------- 164.4/331.9 MB 4.8 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 165.4/331.9 MB 4.8 MB/s eta 0:00:35\n",
      "   -------------------- ------------------- 166.5/331.9 MB 4.8 MB/s eta 0:00:35\n",
      "   -------------------- ------------------- 167.8/331.9 MB 4.8 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 169.1/331.9 MB 4.8 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 170.1/331.9 MB 4.8 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 171.4/331.9 MB 4.8 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 172.5/331.9 MB 4.8 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 173.8/331.9 MB 4.8 MB/s eta 0:00:34\n",
      "   --------------------- ------------------ 175.1/331.9 MB 4.8 MB/s eta 0:00:33\n",
      "   --------------------- ------------------ 176.2/331.9 MB 4.8 MB/s eta 0:00:33\n",
      "   --------------------- ------------------ 177.2/331.9 MB 4.8 MB/s eta 0:00:33\n",
      "   --------------------- ------------------ 178.5/331.9 MB 4.8 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 179.8/331.9 MB 4.8 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 180.6/331.9 MB 4.8 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 181.9/331.9 MB 4.9 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 183.0/331.9 MB 4.9 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 184.5/331.9 MB 4.9 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 185.3/331.9 MB 4.9 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 186.6/331.9 MB 4.9 MB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 187.4/331.9 MB 4.9 MB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 188.5/331.9 MB 4.9 MB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 189.5/331.9 MB 4.9 MB/s eta 0:00:29\n",
      "   ----------------------- ---------------- 190.8/331.9 MB 4.9 MB/s eta 0:00:29\n",
      "   ----------------------- ---------------- 191.6/331.9 MB 4.9 MB/s eta 0:00:29\n",
      "   ----------------------- ---------------- 192.9/331.9 MB 4.9 MB/s eta 0:00:29\n",
      "   ----------------------- ---------------- 194.8/331.9 MB 5.0 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 195.8/331.9 MB 5.0 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 196.6/331.9 MB 5.0 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 198.2/331.9 MB 5.0 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 199.0/331.9 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 199.8/331.9 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 200.5/331.9 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 201.1/331.9 MB 5.0 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 201.6/331.9 MB 4.9 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 202.4/331.9 MB 4.9 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 203.9/331.9 MB 4.9 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 204.7/331.9 MB 4.9 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 206.0/331.9 MB 4.9 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 206.8/331.9 MB 4.9 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 207.1/331.9 MB 4.9 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 208.1/331.9 MB 4.9 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 208.9/331.9 MB 4.9 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 209.5/331.9 MB 4.9 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 210.2/331.9 MB 4.9 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 210.8/331.9 MB 4.9 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 211.0/331.9 MB 4.9 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 211.3/331.9 MB 4.8 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 211.8/331.9 MB 4.8 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 212.9/331.9 MB 4.8 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 213.6/331.9 MB 4.8 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 215.0/331.9 MB 4.8 MB/s eta 0:00:25\n",
      "   -------------------------- ------------- 216.0/331.9 MB 4.8 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 217.3/331.9 MB 4.9 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 217.8/331.9 MB 4.8 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 218.1/331.9 MB 4.8 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 219.4/331.9 MB 4.8 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 220.2/331.9 MB 4.8 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 221.2/331.9 MB 4.8 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 222.3/331.9 MB 4.8 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 223.3/331.9 MB 4.8 MB/s eta 0:00:23\n",
      "   --------------------------- ------------ 224.4/331.9 MB 4.8 MB/s eta 0:00:23\n",
      "   --------------------------- ------------ 225.7/331.9 MB 4.8 MB/s eta 0:00:23\n",
      "   --------------------------- ------------ 226.8/331.9 MB 4.8 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 228.1/331.9 MB 4.8 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 228.9/331.9 MB 4.8 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 229.9/331.9 MB 4.8 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 230.7/331.9 MB 4.8 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 232.0/331.9 MB 4.8 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 233.3/331.9 MB 4.8 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 234.4/331.9 MB 4.8 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 235.4/331.9 MB 4.8 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 235.9/331.9 MB 4.8 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 237.2/331.9 MB 4.8 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 238.6/331.9 MB 4.8 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 239.6/331.9 MB 4.8 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 239.9/331.9 MB 4.8 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 241.7/331.9 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 242.2/331.9 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 242.7/331.9 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 243.5/331.9 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 243.8/331.9 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 244.6/331.9 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 245.9/331.9 MB 4.8 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 246.9/331.9 MB 4.8 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 248.0/331.9 MB 4.8 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 248.8/331.9 MB 4.8 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 249.3/331.9 MB 4.8 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 250.1/331.9 MB 4.7 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 250.9/331.9 MB 4.7 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 251.9/331.9 MB 4.7 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 253.0/331.9 MB 4.7 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 254.5/331.9 MB 4.8 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 255.1/331.9 MB 4.7 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 255.6/331.9 MB 4.7 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 256.6/331.9 MB 4.7 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 257.9/331.9 MB 4.8 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 258.7/331.9 MB 4.8 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 260.0/331.9 MB 4.8 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 260.8/331.9 MB 4.8 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 262.1/331.9 MB 4.8 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 263.2/331.9 MB 4.8 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 264.5/331.9 MB 4.8 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 265.6/331.9 MB 4.8 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 266.6/331.9 MB 4.8 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 267.6/331.9 MB 4.8 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 269.0/331.9 MB 4.8 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 270.0/331.9 MB 4.8 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 271.1/331.9 MB 4.8 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 272.4/331.9 MB 4.8 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 273.4/331.9 MB 4.8 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 274.5/331.9 MB 4.8 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 275.3/331.9 MB 4.8 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 276.3/331.9 MB 4.8 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 277.3/331.9 MB 4.8 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 277.9/331.9 MB 4.8 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 279.2/331.9 MB 4.8 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 280.2/331.9 MB 4.8 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 280.8/331.9 MB 4.8 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 281.8/331.9 MB 4.8 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 282.9/331.9 MB 4.8 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 283.1/331.9 MB 4.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 284.2/331.9 MB 4.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 285.0/331.9 MB 4.7 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 285.7/331.9 MB 4.7 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 287.0/331.9 MB 4.7 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 287.8/331.9 MB 4.7 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 289.1/331.9 MB 4.7 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 289.9/331.9 MB 4.7 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 291.2/331.9 MB 4.7 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 292.0/331.9 MB 4.7 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 293.1/331.9 MB 4.7 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 294.1/331.9 MB 4.7 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 294.6/331.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 295.7/331.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 296.5/331.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 297.5/331.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 298.6/331.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 299.9/331.9 MB 4.7 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 300.7/331.9 MB 4.7 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 301.5/331.9 MB 4.7 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 302.0/331.9 MB 4.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 302.8/331.9 MB 4.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 303.8/331.9 MB 4.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 305.1/331.9 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 305.9/331.9 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 306.7/331.9 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 307.5/331.9 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 308.8/331.9 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 309.1/331.9 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 309.6/331.9 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 310.6/331.9 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 311.7/331.9 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 312.5/331.9 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 313.3/331.9 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 314.3/331.9 MB 4.5 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 315.6/331.9 MB 4.5 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 316.1/331.9 MB 4.5 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 317.2/331.9 MB 4.5 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 318.0/331.9 MB 4.5 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 318.8/331.9 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 319.6/331.9 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 320.6/331.9 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 321.9/331.9 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 322.7/331.9 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 323.5/331.9 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  324.5/331.9 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  325.6/331.9 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  326.6/331.9 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  327.7/331.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  328.7/331.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  329.8/331.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  330.6/331.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  331.9/331.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  331.9/331.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  331.9/331.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  331.9/331.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 331.9/331.9 MB 4.4 MB/s  0:01:10\n",
      "Downloading ml_dtypes-0.5.4-cp312-cp312-win_amd64.whl (212 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.8/5.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.1/5.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.2/5.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 5.0 MB/s  0:00:01\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.5 MB/s  0:00:00\n",
      "Downloading matplotlib-3.10.7-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 6.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.8/8.1 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.9/8.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.9/8.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.8/8.1 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 4.6 MB/s  0:00:01\n",
      "Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/8.7 MB 5.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.8/8.7 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.6/8.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.7/8.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.9/8.7 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.5/8.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.0/8.7 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.8/8.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.6/8.7 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.9/8.7 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.4/8.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 2.8 MB/s  0:00:03\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.61.0-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.6/2.3 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 5.7 MB/s  0:00:00\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp312-cp312-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/2.9 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.3/2.9 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 3.9 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/26.4 MB 6.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/26.4 MB 6.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.2/26.4 MB 7.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.0/26.4 MB 7.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.6/26.4 MB 7.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.9/26.4 MB 7.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.4/26.4 MB 6.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 11.0/26.4 MB 6.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 12.1/26.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 13.1/26.4 MB 6.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 14.4/26.4 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 15.5/26.4 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.5/26.4 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.8/26.4 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 18.9/26.4 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 20.2/26.4 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.3/26.4 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 23.1/26.4 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.4/26.4 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.4/26.4 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 5.7 MB/s  0:00:04\n",
      "Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading pillow-12.0.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/7.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.8/7.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.6/7.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.1/7.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.0/7.0 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.8/7.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 4.3 MB/s  0:00:01\n",
      "Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/38.6 MB 9.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 2.6/38.6 MB 7.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 4.2/38.6 MB 8.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 5.0/38.6 MB 6.4 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 5.5/38.6 MB 5.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 6.0/38.6 MB 5.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.3/38.6 MB 5.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 7.3/38.6 MB 4.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 8.4/38.6 MB 4.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 8.9/38.6 MB 4.4 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 9.4/38.6 MB 4.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 10.5/38.6 MB 4.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 11.5/38.6 MB 4.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 12.1/38.6 MB 4.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 12.8/38.6 MB 4.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 14.2/38.6 MB 4.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 14.4/38.6 MB 4.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 15.5/38.6 MB 4.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 16.5/38.6 MB 4.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 17.0/38.6 MB 4.0 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 17.8/38.6 MB 4.0 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 18.1/38.6 MB 3.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 18.6/38.6 MB 3.8 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 19.1/38.6 MB 3.8 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 20.4/38.6 MB 3.9 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 21.0/38.6 MB 3.9 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 21.8/38.6 MB 3.8 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 22.5/38.6 MB 3.8 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 23.1/38.6 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 23.6/38.6 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 23.9/38.6 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 24.6/38.6 MB 3.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 25.4/38.6 MB 3.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 25.7/38.6 MB 3.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 25.7/38.6 MB 3.6 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 26.2/38.6 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 26.5/38.6 MB 3.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 27.0/38.6 MB 3.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 27.8/38.6 MB 3.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 28.0/38.6 MB 3.3 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 28.3/38.6 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 29.1/38.6 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 29.9/38.6 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 30.9/38.6 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 31.7/38.6 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 33.0/38.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.3/38.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.1/38.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.6/38.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 35.1/38.6 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 35.7/38.6 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.9/38.6 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 3.3 MB/s  0:00:11\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading wrapt-2.0.1-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp312-cp312-win_amd64.whl (312 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, threadpoolctl, termcolor, tensorboard-data-server, scipy, pillow, optree, opt_einsum, ml_dtypes, mdurl, markdown, kiwisolver, h5py, google_pasta, gast, fonttools, cycler, contourpy, absl-py, tensorboard, scikit-learn, matplotlib, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "\n",
      "   - --------------------------------------  1/31 [libclang]\n",
      "   - --------------------------------------  1/31 [libclang]\n",
      "   --- ------------------------------------  3/31 [wrapt]\n",
      "   ------ ---------------------------------  5/31 [threadpoolctl]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ---------- -----------------------------  8/31 [scipy]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ------------ --------------------------- 10/31 [optree]\n",
      "   --------------- ------------------------ 12/31 [ml_dtypes]\n",
      "   ------------------ --------------------- 14/31 [markdown]\n",
      "   -------------------- ------------------- 16/31 [h5py]\n",
      "   -------------------- ------------------- 16/31 [h5py]\n",
      "   -------------------- ------------------- 16/31 [h5py]\n",
      "   --------------------- ------------------ 17/31 [google_pasta]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   ------------------------ --------------- 19/31 [fonttools]\n",
      "   --------------------------- ------------ 21/31 [contourpy]\n",
      "   ---------------------------- ----------- 22/31 [absl-py]\n",
      "   ----------------------------- ---------- 23/31 [tensorboard]\n",
      "   ----------------------------- ---------- 23/31 [tensorboard]\n",
      "   ----------------------------- ---------- 23/31 [tensorboard]\n",
      "   ----------------------------- ---------- 23/31 [tensorboard]\n",
      "   ----------------------------- ---------- 23/31 [tensorboard]\n",
      "   ----------------------------- ---------- 23/31 [tensorboard]\n",
      "   ----------------------------- ---------- 23/31 [tensorboard]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   ------------------------------ --------- 24/31 [scikit-learn]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   -------------------------------- ------- 25/31 [matplotlib]\n",
      "   --------------------------------- ------ 26/31 [markdown-it-py]\n",
      "   --------------------------------- ------ 26/31 [markdown-it-py]\n",
      "   ---------------------------------- ----- 27/31 [astunparse]\n",
      "   ------------------------------------ --- 28/31 [rich]\n",
      "   ------------------------------------ --- 28/31 [rich]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   ------------------------------------- -- 29/31 [keras]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   -------------------------------------- - 30/31 [tensorflow]\n",
      "   ---------------------------------------- 31/31 [tensorflow]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 contourpy-1.3.3 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.61.0 gast-0.7.0 google_pasta-0.2.0 h5py-3.15.1 keras-3.12.0 kiwisolver-1.4.9 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 matplotlib-3.10.7 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 pillow-12.0.0 rich-14.2.0 scikit-learn-1.7.2 scipy-1.16.3 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 threadpoolctl-3.6.0 wheel-0.45.1 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras numpy matplotlib scikit-learn opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab587b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, numpy as np, joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "CHARS74K_ROOT = r\"English/Img/GoodImg/Bmp\"  # มี Sample001, Sample002, ... 62 folders [web:50]\n",
    "\n",
    "IMG_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543492d",
   "metadata": {},
   "source": [
    "โหลดรูปจาก Chars74K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fe53f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7011, 32, 32, 3) (7011,)\n"
     ]
    }
   ],
   "source": [
    "def load_chars74k(root_dir, limit_per_class=None):\n",
    "    X, y = [], []\n",
    "    class_dirs = sorted(os.listdir(root_dir))  # Sample001, Sample002, ...\n",
    "    \n",
    "    for cdir in class_dirs:\n",
    "        cpath = os.path.join(root_dir, cdir)\n",
    "        if not os.path.isdir(cpath):\n",
    "            continue\n",
    "        \n",
    "        files = [f for f in os.listdir(cpath) if f.lower().endswith('.png')]\n",
    "        if limit_per_class:\n",
    "            files = files[:limit_per_class]\n",
    "        \n",
    "        # label เป็นตัวอักษรจากชื่อไฟล์ เช่น 'img001-00001.png' → ใช้ตัวแรกในชื่อ class mapping ภายหลัง\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(cpath, fname)\n",
    "            img = cv2.imread(fpath)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            X.append(img)\n",
    "            # label เป็นชื่อโฟลเดอร์ (Sample0xx) ไว้ไป map ต่อทีหลัง\n",
    "            y.append(cdir)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y_folder = load_chars74k(CHARS74K_ROOT, limit_per_class=300)  # ให้เร็วขึ้น\n",
    "print(X.shape, y_folder.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f3a6b",
   "metadata": {},
   "source": [
    "map โฟลเดอร์เป็นตัวอักษร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'\n",
      " 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z'\n",
      " 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r'\n",
      " 's' 't' 'u' 'v' 'w' 'x' 'y' 'z'] Total: 62\n",
      "(5608, 32, 32, 3) (1403, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FOLDER_TO_CHAR = {\n",
    "    \"Sample001\": \"0\",\n",
    "    \"Sample002\": \"1\",\n",
    "    \"Sample003\": \"2\",\n",
    "    \"Sample004\": \"3\",\n",
    "    \"Sample005\": \"4\",\n",
    "    \"Sample006\": \"5\",\n",
    "    \"Sample007\": \"6\",\n",
    "    \"Sample008\": \"7\",\n",
    "    \"Sample009\": \"8\",\n",
    "    \"Sample010\": \"9\",\n",
    "    \"Sample011\": \"A\",\n",
    "    \"Sample012\": \"B\",\n",
    "    \"Sample013\": \"C\",\n",
    "    \"Sample014\": \"D\",\n",
    "    \"Sample015\": \"E\",\n",
    "    \"Sample016\": \"F\",\n",
    "    \"Sample017\": \"G\",\n",
    "    \"Sample018\": \"H\",\n",
    "    \"Sample019\": \"I\",\n",
    "    \"Sample020\": \"J\",\n",
    "    \"Sample021\": \"K\",\n",
    "    \"Sample022\": \"L\",\n",
    "    \"Sample023\": \"M\",\n",
    "    \"Sample024\": \"N\",\n",
    "    \"Sample025\": \"O\",\n",
    "    \"Sample026\": \"P\",\n",
    "    \"Sample027\": \"Q\",\n",
    "    \"Sample028\": \"R\",\n",
    "    \"Sample029\": \"S\",\n",
    "    \"Sample030\": \"T\",\n",
    "    \"Sample031\": \"U\",\n",
    "    \"Sample032\": \"V\",\n",
    "    \"Sample033\": \"W\",\n",
    "    \"Sample034\": \"X\",\n",
    "    \"Sample035\": \"Y\",\n",
    "    \"Sample036\": \"Z\",\n",
    "    \"Sample037\": \"a\",\n",
    "    \"Sample038\": \"b\",\n",
    "    \"Sample039\": \"c\",\n",
    "    \"Sample040\": \"d\",\n",
    "    \"Sample041\": \"e\",\n",
    "    \"Sample042\": \"f\",\n",
    "    \"Sample043\": \"g\",\n",
    "    \"Sample044\": \"h\",\n",
    "    \"Sample045\": \"i\",\n",
    "    \"Sample046\": \"j\",\n",
    "    \"Sample047\": \"k\",\n",
    "    \"Sample048\": \"l\",\n",
    "    \"Sample049\": \"m\",\n",
    "    \"Sample050\": \"n\",\n",
    "    \"Sample051\": \"o\",\n",
    "    \"Sample052\": \"p\",\n",
    "    \"Sample053\": \"q\",\n",
    "    \"Sample054\": \"r\",\n",
    "    \"Sample055\": \"s\",\n",
    "    \"Sample056\": \"t\",\n",
    "    \"Sample057\": \"u\",\n",
    "    \"Sample058\": \"v\",\n",
    "    \"Sample059\": \"w\",\n",
    "    \"Sample060\": \"x\",\n",
    "    \"Sample061\": \"y\",\n",
    "    \"Sample062\": \"z\"\n",
    "    \n",
    "}\n",
    "\n",
    "y_chars = np.array([FOLDER_TO_CHAR[f] for f in y_folder])\n",
    "\n",
    "# encode เป็นตัวเลข 0..C-1\n",
    "le = LabelEncoder()\n",
    "y_idx = le.fit_transform(y_chars)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", le.classes_, \"Total:\", num_classes)\n",
    "\n",
    "\n",
    "X = X.astype(\"float32\") / 255.0\n",
    "y_onehot = to_categorical(y_idx, num_classes=num_classes)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_onehot, test_size=0.2, random_state=42, stratify=y_idx)\n",
    "print(X_train.shape, X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd2b49",
   "metadata": {},
   "source": [
    " สร้างและฝึก CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec47e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.0380 - loss: 3.9582 - val_accuracy: 0.0428 - val_loss: 3.8869\n",
      "Epoch 2/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.0403 - loss: 3.8958 - val_accuracy: 0.0442 - val_loss: 3.8739\n",
      "Epoch 3/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.0394 - loss: 3.8907 - val_accuracy: 0.0428 - val_loss: 3.8565\n",
      "Epoch 4/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.0451 - loss: 3.8760 - val_accuracy: 0.0492 - val_loss: 3.8273\n",
      "Epoch 5/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.1307 - loss: 3.5895 - val_accuracy: 0.3414 - val_loss: 2.8486\n",
      "Epoch 6/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3445 - loss: 2.5545 - val_accuracy: 0.5403 - val_loss: 1.9467\n",
      "Epoch 7/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.4982 - loss: 1.8195 - val_accuracy: 0.6614 - val_loss: 1.2015\n",
      "Epoch 8/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.5949 - loss: 1.4050 - val_accuracy: 0.7099 - val_loss: 1.0020\n",
      "Epoch 9/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.6505 - loss: 1.2002 - val_accuracy: 0.7484 - val_loss: 0.8467\n",
      "Epoch 10/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.6847 - loss: 1.0585 - val_accuracy: 0.7876 - val_loss: 0.7663\n",
      "Epoch 11/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7099 - loss: 0.9464 - val_accuracy: 0.7805 - val_loss: 0.7164\n",
      "Epoch 12/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7359 - loss: 0.8702 - val_accuracy: 0.8033 - val_loss: 0.6556\n",
      "Epoch 13/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7498 - loss: 0.7950 - val_accuracy: 0.8011 - val_loss: 0.6488\n",
      "Epoch 14/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7607 - loss: 0.7454 - val_accuracy: 0.8154 - val_loss: 0.5994\n",
      "Epoch 15/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7759 - loss: 0.6972 - val_accuracy: 0.8083 - val_loss: 0.6245\n",
      "Epoch 16/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7891 - loss: 0.6634 - val_accuracy: 0.8104 - val_loss: 0.6123\n",
      "Epoch 17/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7883 - loss: 0.6291 - val_accuracy: 0.8133 - val_loss: 0.5926\n",
      "Epoch 18/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8035 - loss: 0.5944 - val_accuracy: 0.8261 - val_loss: 0.5578\n",
      "Epoch 19/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8103 - loss: 0.5678 - val_accuracy: 0.8168 - val_loss: 0.5660\n",
      "Epoch 20/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8140 - loss: 0.5448 - val_accuracy: 0.8218 - val_loss: 0.5760\n",
      "Epoch 21/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8172 - loss: 0.5374 - val_accuracy: 0.8211 - val_loss: 0.5541\n",
      "Epoch 22/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8295 - loss: 0.4941 - val_accuracy: 0.8147 - val_loss: 0.5849\n",
      "Epoch 23/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8301 - loss: 0.4880 - val_accuracy: 0.8232 - val_loss: 0.5571\n",
      "Epoch 24/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8370 - loss: 0.4616 - val_accuracy: 0.8268 - val_loss: 0.5643\n",
      "Epoch 25/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8361 - loss: 0.4533 - val_accuracy: 0.8325 - val_loss: 0.5545\n",
      "Epoch 26/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8532 - loss: 0.4255 - val_accuracy: 0.8239 - val_loss: 0.5823\n",
      "Epoch 27/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.8461 - loss: 0.4279 - val_accuracy: 0.8339 - val_loss: 0.5462\n",
      "Epoch 28/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8511 - loss: 0.4010 - val_accuracy: 0.8346 - val_loss: 0.5983\n",
      "Epoch 29/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8602 - loss: 0.3891 - val_accuracy: 0.8354 - val_loss: 0.5469\n",
      "Epoch 30/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8575 - loss: 0.3960 - val_accuracy: 0.8311 - val_loss: 0.5810\n",
      "Epoch 31/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8698 - loss: 0.3639 - val_accuracy: 0.8354 - val_loss: 0.5619\n",
      "Epoch 32/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8727 - loss: 0.3529 - val_accuracy: 0.8453 - val_loss: 0.5594\n",
      "Epoch 33/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8691 - loss: 0.3531 - val_accuracy: 0.8375 - val_loss: 0.5699\n",
      "Epoch 34/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.8773 - loss: 0.3472 - val_accuracy: 0.8354 - val_loss: 0.5871\n",
      "Epoch 35/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.8827 - loss: 0.3243 - val_accuracy: 0.8396 - val_loss: 0.6050\n",
      "Epoch 36/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.8834 - loss: 0.3203 - val_accuracy: 0.8254 - val_loss: 0.5928\n",
      "Epoch 37/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.8811 - loss: 0.3284 - val_accuracy: 0.8289 - val_loss: 0.6096\n",
      "Epoch 38/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.8827 - loss: 0.3236 - val_accuracy: 0.8346 - val_loss: 0.5774\n",
      "Epoch 39/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8800 - loss: 0.3153 - val_accuracy: 0.8389 - val_loss: 0.5674\n",
      "Epoch 40/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8820 - loss: 0.3154 - val_accuracy: 0.8339 - val_loss: 0.5947\n",
      "Epoch 41/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8902 - loss: 0.2834 - val_accuracy: 0.8403 - val_loss: 0.5850\n",
      "Epoch 42/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8930 - loss: 0.2929 - val_accuracy: 0.8468 - val_loss: 0.5955\n",
      "Epoch 43/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8923 - loss: 0.2992 - val_accuracy: 0.8368 - val_loss: 0.5780\n",
      "Epoch 44/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8935 - loss: 0.2745 - val_accuracy: 0.8325 - val_loss: 0.6170\n",
      "Epoch 45/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8989 - loss: 0.2778 - val_accuracy: 0.8289 - val_loss: 0.6033\n",
      "Epoch 46/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8886 - loss: 0.2901 - val_accuracy: 0.8339 - val_loss: 0.5895\n",
      "Epoch 47/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8994 - loss: 0.2599 - val_accuracy: 0.8382 - val_loss: 0.5748\n",
      "Epoch 48/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9028 - loss: 0.2557 - val_accuracy: 0.8346 - val_loss: 0.6151\n",
      "Epoch 49/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9025 - loss: 0.2688 - val_accuracy: 0.8475 - val_loss: 0.5889\n",
      "Epoch 50/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9041 - loss: 0.2496 - val_accuracy: 0.8318 - val_loss: 0.6169\n",
      "Epoch 51/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9094 - loss: 0.2484 - val_accuracy: 0.8432 - val_loss: 0.6045\n",
      "Epoch 52/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9119 - loss: 0.2462 - val_accuracy: 0.8560 - val_loss: 0.5636\n",
      "Epoch 53/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9103 - loss: 0.2363 - val_accuracy: 0.8446 - val_loss: 0.6126\n",
      "Epoch 54/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9112 - loss: 0.2302 - val_accuracy: 0.8460 - val_loss: 0.5960\n",
      "Epoch 55/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9176 - loss: 0.2261 - val_accuracy: 0.8532 - val_loss: 0.6128\n",
      "Epoch 56/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9182 - loss: 0.2255 - val_accuracy: 0.8439 - val_loss: 0.6073\n",
      "Epoch 57/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9205 - loss: 0.2264 - val_accuracy: 0.8346 - val_loss: 0.6381\n",
      "Epoch 58/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9199 - loss: 0.2195 - val_accuracy: 0.8403 - val_loss: 0.6488\n",
      "Epoch 59/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.2220 - val_accuracy: 0.8446 - val_loss: 0.5984\n",
      "Epoch 60/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9221 - loss: 0.2091 - val_accuracy: 0.8425 - val_loss: 0.6510\n",
      "Epoch 61/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9201 - loss: 0.2079 - val_accuracy: 0.8510 - val_loss: 0.6470\n",
      "Epoch 62/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.9233 - loss: 0.2097 - val_accuracy: 0.8418 - val_loss: 0.6802\n",
      "Epoch 63/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.9231 - loss: 0.2118 - val_accuracy: 0.8332 - val_loss: 0.6038\n",
      "Epoch 64/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9189 - loss: 0.2231 - val_accuracy: 0.8375 - val_loss: 0.6091\n",
      "Epoch 65/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9214 - loss: 0.2010 - val_accuracy: 0.8403 - val_loss: 0.6211\n",
      "Epoch 66/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9151 - loss: 0.2240 - val_accuracy: 0.8453 - val_loss: 0.6373\n",
      "Epoch 67/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9244 - loss: 0.1934 - val_accuracy: 0.8382 - val_loss: 0.6593\n",
      "Epoch 68/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9335 - loss: 0.1791 - val_accuracy: 0.8496 - val_loss: 0.6904\n",
      "Epoch 69/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9201 - loss: 0.2037 - val_accuracy: 0.8453 - val_loss: 0.6520\n",
      "Epoch 70/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9276 - loss: 0.1973 - val_accuracy: 0.8482 - val_loss: 0.6247\n",
      "Epoch 71/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9290 - loss: 0.2014 - val_accuracy: 0.8510 - val_loss: 0.6115\n",
      "Epoch 72/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9319 - loss: 0.1938 - val_accuracy: 0.8446 - val_loss: 0.6366\n",
      "Epoch 73/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9322 - loss: 0.1875 - val_accuracy: 0.8418 - val_loss: 0.6302\n",
      "Epoch 74/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9299 - loss: 0.1912 - val_accuracy: 0.8403 - val_loss: 0.6747\n",
      "Epoch 75/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9344 - loss: 0.1761 - val_accuracy: 0.8475 - val_loss: 0.5964\n",
      "Epoch 76/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9321 - loss: 0.1801 - val_accuracy: 0.8418 - val_loss: 0.6269\n",
      "Epoch 77/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9292 - loss: 0.1969 - val_accuracy: 0.8460 - val_loss: 0.7073\n",
      "Epoch 78/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9347 - loss: 0.1844 - val_accuracy: 0.8468 - val_loss: 0.6369\n",
      "Epoch 79/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9338 - loss: 0.1804 - val_accuracy: 0.8574 - val_loss: 0.6589\n",
      "Epoch 80/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9387 - loss: 0.1611 - val_accuracy: 0.8560 - val_loss: 0.6842\n",
      "Epoch 81/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9333 - loss: 0.1795 - val_accuracy: 0.8510 - val_loss: 0.6391\n",
      "Epoch 82/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9388 - loss: 0.1713 - val_accuracy: 0.8411 - val_loss: 0.6466\n",
      "Epoch 83/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9340 - loss: 0.1758 - val_accuracy: 0.8482 - val_loss: 0.6746\n",
      "Epoch 84/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9369 - loss: 0.1761 - val_accuracy: 0.8560 - val_loss: 0.6619\n",
      "Epoch 85/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9381 - loss: 0.1710 - val_accuracy: 0.8432 - val_loss: 0.6826\n",
      "Epoch 86/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9424 - loss: 0.1587 - val_accuracy: 0.8489 - val_loss: 0.6711\n",
      "Epoch 87/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9422 - loss: 0.1654 - val_accuracy: 0.8503 - val_loss: 0.6710\n",
      "Epoch 88/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9401 - loss: 0.1591 - val_accuracy: 0.8546 - val_loss: 0.6397\n",
      "Epoch 89/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9388 - loss: 0.1610 - val_accuracy: 0.8425 - val_loss: 0.6464\n",
      "Epoch 90/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9413 - loss: 0.1716 - val_accuracy: 0.8517 - val_loss: 0.6871\n",
      "Epoch 91/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9428 - loss: 0.1573 - val_accuracy: 0.8475 - val_loss: 0.6861\n",
      "Epoch 92/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.9385 - loss: 0.1624 - val_accuracy: 0.8610 - val_loss: 0.6712\n",
      "Epoch 93/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9403 - loss: 0.1545 - val_accuracy: 0.8411 - val_loss: 0.6971\n",
      "Epoch 94/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9385 - loss: 0.1603 - val_accuracy: 0.8489 - val_loss: 0.6979\n",
      "Epoch 95/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9369 - loss: 0.1721 - val_accuracy: 0.8567 - val_loss: 0.6507\n",
      "Epoch 96/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9435 - loss: 0.1590 - val_accuracy: 0.8446 - val_loss: 0.6494\n",
      "Epoch 97/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9465 - loss: 0.1444 - val_accuracy: 0.8389 - val_loss: 0.7055\n",
      "Epoch 98/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9419 - loss: 0.1679 - val_accuracy: 0.8503 - val_loss: 0.6692\n",
      "Epoch 99/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9413 - loss: 0.1525 - val_accuracy: 0.8546 - val_loss: 0.7201\n",
      "Epoch 100/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9488 - loss: 0.1367 - val_accuracy: 0.8510 - val_loss: 0.7197\n",
      "Epoch 101/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9437 - loss: 0.1578 - val_accuracy: 0.8567 - val_loss: 0.6709\n",
      "Epoch 102/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9444 - loss: 0.1506 - val_accuracy: 0.8489 - val_loss: 0.6619\n",
      "Epoch 103/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9519 - loss: 0.1375 - val_accuracy: 0.8482 - val_loss: 0.6858\n",
      "Epoch 104/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9517 - loss: 0.1275 - val_accuracy: 0.8482 - val_loss: 0.7483\n",
      "Epoch 105/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9502 - loss: 0.1305 - val_accuracy: 0.8396 - val_loss: 0.7568\n",
      "Epoch 106/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9474 - loss: 0.1407 - val_accuracy: 0.8489 - val_loss: 0.7085\n",
      "Epoch 107/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9469 - loss: 0.1444 - val_accuracy: 0.8503 - val_loss: 0.7360\n",
      "Epoch 108/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9476 - loss: 0.1529 - val_accuracy: 0.8453 - val_loss: 0.7448\n",
      "Epoch 109/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9479 - loss: 0.1520 - val_accuracy: 0.8418 - val_loss: 0.6762\n",
      "Epoch 110/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.9533 - loss: 0.1274 - val_accuracy: 0.8496 - val_loss: 0.7642\n",
      "Epoch 111/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9492 - loss: 0.1401 - val_accuracy: 0.8482 - val_loss: 0.7486\n",
      "Epoch 112/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.9531 - loss: 0.1275 - val_accuracy: 0.8475 - val_loss: 0.7493\n",
      "Epoch 113/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9515 - loss: 0.1239 - val_accuracy: 0.8546 - val_loss: 0.7183\n",
      "Epoch 114/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9549 - loss: 0.1261 - val_accuracy: 0.8546 - val_loss: 0.7261\n",
      "Epoch 115/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9481 - loss: 0.1515 - val_accuracy: 0.8582 - val_loss: 0.6487\n",
      "Epoch 116/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.9577 - loss: 0.1133 - val_accuracy: 0.8560 - val_loss: 0.7519\n",
      "Epoch 117/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9515 - loss: 0.1348 - val_accuracy: 0.8517 - val_loss: 0.7298\n",
      "Epoch 118/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9527 - loss: 0.1270 - val_accuracy: 0.8525 - val_loss: 0.7238\n",
      "Epoch 119/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.9572 - loss: 0.1214 - val_accuracy: 0.8624 - val_loss: 0.7060\n",
      "Epoch 120/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.9536 - loss: 0.1408 - val_accuracy: 0.8525 - val_loss: 0.7267\n",
      "Epoch 121/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9504 - loss: 0.1375 - val_accuracy: 0.8525 - val_loss: 0.7090\n",
      "Epoch 122/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.9501 - loss: 0.1409 - val_accuracy: 0.8546 - val_loss: 0.6866\n",
      "Epoch 123/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.9535 - loss: 0.1278 - val_accuracy: 0.8560 - val_loss: 0.7208\n",
      "Epoch 124/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9595 - loss: 0.1229 - val_accuracy: 0.8496 - val_loss: 0.7188\n",
      "Epoch 125/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9561 - loss: 0.1211 - val_accuracy: 0.8546 - val_loss: 0.6843\n",
      "Epoch 126/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9583 - loss: 0.1169 - val_accuracy: 0.8489 - val_loss: 0.7223\n",
      "Epoch 127/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9606 - loss: 0.1145 - val_accuracy: 0.8425 - val_loss: 0.7030\n",
      "Epoch 128/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9561 - loss: 0.1189 - val_accuracy: 0.8482 - val_loss: 0.7485\n",
      "Epoch 129/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9631 - loss: 0.1138 - val_accuracy: 0.8553 - val_loss: 0.6941\n",
      "Epoch 130/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9549 - loss: 0.1273 - val_accuracy: 0.8475 - val_loss: 0.7468\n",
      "Epoch 131/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9585 - loss: 0.1194 - val_accuracy: 0.8468 - val_loss: 0.7631\n",
      "Epoch 132/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9615 - loss: 0.1122 - val_accuracy: 0.8475 - val_loss: 0.7478\n",
      "Epoch 133/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9556 - loss: 0.1166 - val_accuracy: 0.8539 - val_loss: 0.7566\n",
      "Epoch 134/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9577 - loss: 0.1269 - val_accuracy: 0.8503 - val_loss: 0.6920\n",
      "Epoch 135/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9604 - loss: 0.1203 - val_accuracy: 0.8510 - val_loss: 0.7682\n",
      "Epoch 136/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9502 - loss: 0.1268 - val_accuracy: 0.8596 - val_loss: 0.7151\n",
      "Epoch 137/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9668 - loss: 0.0967 - val_accuracy: 0.8574 - val_loss: 0.7775\n",
      "Epoch 138/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9558 - loss: 0.1245 - val_accuracy: 0.8489 - val_loss: 0.7698\n",
      "Epoch 139/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9597 - loss: 0.1110 - val_accuracy: 0.8489 - val_loss: 0.7304\n",
      "Epoch 140/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9510 - loss: 0.1368 - val_accuracy: 0.8525 - val_loss: 0.7979\n",
      "Epoch 141/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9508 - loss: 0.1517 - val_accuracy: 0.8453 - val_loss: 0.7535\n",
      "Epoch 142/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9599 - loss: 0.1180 - val_accuracy: 0.8560 - val_loss: 0.7341\n",
      "Epoch 143/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9618 - loss: 0.1080 - val_accuracy: 0.8589 - val_loss: 0.7857\n",
      "Epoch 144/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9579 - loss: 0.1135 - val_accuracy: 0.8503 - val_loss: 0.7754\n",
      "Epoch 145/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9597 - loss: 0.1067 - val_accuracy: 0.8389 - val_loss: 0.7298\n",
      "Epoch 146/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9581 - loss: 0.1187 - val_accuracy: 0.8582 - val_loss: 0.7435\n",
      "Epoch 147/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9645 - loss: 0.0980 - val_accuracy: 0.8439 - val_loss: 0.7379\n",
      "Epoch 148/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.9522 - loss: 0.1367 - val_accuracy: 0.8553 - val_loss: 0.7259\n",
      "Epoch 149/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9622 - loss: 0.1075 - val_accuracy: 0.8468 - val_loss: 0.7684\n",
      "Epoch 150/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9622 - loss: 0.1113 - val_accuracy: 0.8517 - val_loss: 0.7262\n",
      "Epoch 151/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9643 - loss: 0.1029 - val_accuracy: 0.8510 - val_loss: 0.8499\n",
      "Epoch 152/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9620 - loss: 0.1107 - val_accuracy: 0.8517 - val_loss: 0.8700\n",
      "Epoch 153/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9590 - loss: 0.1186 - val_accuracy: 0.8503 - val_loss: 0.8081\n",
      "Epoch 154/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9602 - loss: 0.1115 - val_accuracy: 0.8439 - val_loss: 0.7854\n",
      "Epoch 155/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9631 - loss: 0.1098 - val_accuracy: 0.8460 - val_loss: 0.8085\n",
      "Epoch 156/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9663 - loss: 0.0885 - val_accuracy: 0.8560 - val_loss: 0.8411\n",
      "Epoch 157/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9645 - loss: 0.1016 - val_accuracy: 0.8603 - val_loss: 0.8236\n",
      "Epoch 158/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9615 - loss: 0.1131 - val_accuracy: 0.8603 - val_loss: 0.8119\n",
      "Epoch 159/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9595 - loss: 0.1183 - val_accuracy: 0.8567 - val_loss: 0.7823\n",
      "Epoch 160/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9592 - loss: 0.1136 - val_accuracy: 0.8475 - val_loss: 0.7900\n",
      "Epoch 161/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9624 - loss: 0.1022 - val_accuracy: 0.8460 - val_loss: 0.7886\n",
      "Epoch 162/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9640 - loss: 0.1109 - val_accuracy: 0.8439 - val_loss: 0.7998\n",
      "Epoch 163/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9634 - loss: 0.1043 - val_accuracy: 0.8546 - val_loss: 0.7675\n",
      "Epoch 164/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9638 - loss: 0.1060 - val_accuracy: 0.8468 - val_loss: 0.7745\n",
      "Epoch 165/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9611 - loss: 0.1072 - val_accuracy: 0.8468 - val_loss: 0.7313\n",
      "Epoch 166/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9634 - loss: 0.1037 - val_accuracy: 0.8496 - val_loss: 0.8178\n",
      "Epoch 167/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9604 - loss: 0.1077 - val_accuracy: 0.8510 - val_loss: 0.7861\n",
      "Epoch 168/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9629 - loss: 0.1084 - val_accuracy: 0.8425 - val_loss: 0.7447\n",
      "Epoch 169/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9629 - loss: 0.1043 - val_accuracy: 0.8525 - val_loss: 0.7614\n",
      "Epoch 170/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9643 - loss: 0.1021 - val_accuracy: 0.8432 - val_loss: 0.7756\n",
      "Epoch 171/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9670 - loss: 0.0973 - val_accuracy: 0.8574 - val_loss: 0.7303\n",
      "Epoch 172/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9629 - loss: 0.1098 - val_accuracy: 0.8624 - val_loss: 0.7754\n",
      "Epoch 173/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.9642 - loss: 0.1203 - val_accuracy: 0.8560 - val_loss: 0.7202\n",
      "Epoch 174/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.9593 - loss: 0.1157 - val_accuracy: 0.8532 - val_loss: 0.7791\n",
      "Epoch 175/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.9661 - loss: 0.1005 - val_accuracy: 0.8539 - val_loss: 0.8018\n",
      "Epoch 176/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9686 - loss: 0.0923 - val_accuracy: 0.8582 - val_loss: 0.7886\n",
      "Epoch 177/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.9659 - loss: 0.0961 - val_accuracy: 0.8574 - val_loss: 0.7880\n",
      "Epoch 178/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9652 - loss: 0.0997 - val_accuracy: 0.8574 - val_loss: 0.7655\n",
      "Epoch 179/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.9674 - loss: 0.0911 - val_accuracy: 0.8475 - val_loss: 0.8348\n",
      "Epoch 180/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9638 - loss: 0.1082 - val_accuracy: 0.8525 - val_loss: 0.8407\n",
      "Epoch 181/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9588 - loss: 0.1170 - val_accuracy: 0.8553 - val_loss: 0.8204\n",
      "Epoch 182/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9697 - loss: 0.0841 - val_accuracy: 0.8539 - val_loss: 0.8295\n",
      "Epoch 183/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9640 - loss: 0.1024 - val_accuracy: 0.8532 - val_loss: 0.7878\n",
      "Epoch 184/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9693 - loss: 0.0885 - val_accuracy: 0.8517 - val_loss: 0.8372\n",
      "Epoch 185/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9688 - loss: 0.0898 - val_accuracy: 0.8532 - val_loss: 0.8623\n",
      "Epoch 186/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9620 - loss: 0.1154 - val_accuracy: 0.8496 - val_loss: 0.7565\n",
      "Epoch 187/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9690 - loss: 0.0919 - val_accuracy: 0.8475 - val_loss: 0.8028\n",
      "Epoch 188/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9652 - loss: 0.0903 - val_accuracy: 0.8624 - val_loss: 0.8426\n",
      "Epoch 189/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9668 - loss: 0.0873 - val_accuracy: 0.8396 - val_loss: 0.8798\n",
      "Epoch 190/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9672 - loss: 0.1001 - val_accuracy: 0.8546 - val_loss: 0.7478\n",
      "Epoch 191/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9650 - loss: 0.0995 - val_accuracy: 0.8553 - val_loss: 0.8182\n",
      "Epoch 192/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9650 - loss: 0.1032 - val_accuracy: 0.8460 - val_loss: 0.8275\n",
      "Epoch 193/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9672 - loss: 0.0928 - val_accuracy: 0.8539 - val_loss: 0.7989\n",
      "Epoch 194/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9683 - loss: 0.0970 - val_accuracy: 0.8460 - val_loss: 0.7365\n",
      "Epoch 195/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9661 - loss: 0.1026 - val_accuracy: 0.8439 - val_loss: 0.7847\n",
      "Epoch 196/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9699 - loss: 0.0860 - val_accuracy: 0.8539 - val_loss: 0.8690\n",
      "Epoch 197/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9683 - loss: 0.0817 - val_accuracy: 0.8525 - val_loss: 0.8738\n",
      "Epoch 198/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9675 - loss: 0.0953 - val_accuracy: 0.8546 - val_loss: 0.7668\n",
      "Epoch 199/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9679 - loss: 0.0857 - val_accuracy: 0.8539 - val_loss: 0.8614\n",
      "Epoch 200/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9693 - loss: 0.0855 - val_accuracy: 0.8560 - val_loss: 0.8049\n",
      "Epoch 201/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9704 - loss: 0.0852 - val_accuracy: 0.8617 - val_loss: 0.8384\n",
      "Epoch 202/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9708 - loss: 0.0858 - val_accuracy: 0.8425 - val_loss: 0.8328\n",
      "Epoch 203/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9741 - loss: 0.0764 - val_accuracy: 0.8574 - val_loss: 0.8455\n",
      "Epoch 204/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9702 - loss: 0.0913 - val_accuracy: 0.8546 - val_loss: 0.8415\n",
      "Epoch 205/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9679 - loss: 0.0949 - val_accuracy: 0.8532 - val_loss: 0.7949\n",
      "Epoch 206/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9706 - loss: 0.0847 - val_accuracy: 0.8553 - val_loss: 0.7701\n",
      "Epoch 207/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9659 - loss: 0.1131 - val_accuracy: 0.8503 - val_loss: 0.7835\n",
      "Epoch 208/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9690 - loss: 0.0932 - val_accuracy: 0.8446 - val_loss: 0.8108\n",
      "Epoch 209/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9716 - loss: 0.0848 - val_accuracy: 0.8617 - val_loss: 0.7988\n",
      "Epoch 210/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.9647 - loss: 0.0965 - val_accuracy: 0.8539 - val_loss: 0.8078\n",
      "Epoch 211/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.9683 - loss: 0.0862 - val_accuracy: 0.8617 - val_loss: 0.7826\n",
      "Epoch 212/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.9747 - loss: 0.0780 - val_accuracy: 0.8553 - val_loss: 0.8465\n",
      "Epoch 213/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9709 - loss: 0.0836 - val_accuracy: 0.8510 - val_loss: 0.8470\n",
      "Epoch 214/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9704 - loss: 0.0803 - val_accuracy: 0.8525 - val_loss: 0.8730\n",
      "Epoch 215/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9727 - loss: 0.0821 - val_accuracy: 0.8582 - val_loss: 0.8059\n",
      "Epoch 216/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.9659 - loss: 0.1056 - val_accuracy: 0.8532 - val_loss: 0.8004\n",
      "Epoch 217/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9674 - loss: 0.0928 - val_accuracy: 0.8546 - val_loss: 0.7945\n",
      "Epoch 218/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9634 - loss: 0.1014 - val_accuracy: 0.8475 - val_loss: 0.7916\n",
      "Epoch 219/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9709 - loss: 0.0825 - val_accuracy: 0.8510 - val_loss: 0.8056\n",
      "Epoch 220/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9738 - loss: 0.0724 - val_accuracy: 0.8475 - val_loss: 0.9233\n",
      "Epoch 221/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9720 - loss: 0.0745 - val_accuracy: 0.8532 - val_loss: 0.9476\n",
      "Epoch 222/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9670 - loss: 0.1017 - val_accuracy: 0.8460 - val_loss: 0.8888\n",
      "Epoch 223/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9725 - loss: 0.0811 - val_accuracy: 0.8582 - val_loss: 0.8346\n",
      "Epoch 224/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9700 - loss: 0.0916 - val_accuracy: 0.8574 - val_loss: 0.7913\n",
      "Epoch 225/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9724 - loss: 0.0805 - val_accuracy: 0.8539 - val_loss: 0.8151\n",
      "Epoch 226/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9759 - loss: 0.0666 - val_accuracy: 0.8539 - val_loss: 0.8632\n",
      "Epoch 227/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9663 - loss: 0.1017 - val_accuracy: 0.8460 - val_loss: 0.8799\n",
      "Epoch 228/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9715 - loss: 0.0896 - val_accuracy: 0.8439 - val_loss: 0.8624\n",
      "Epoch 229/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9743 - loss: 0.0784 - val_accuracy: 0.8567 - val_loss: 1.0089\n",
      "Epoch 230/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9713 - loss: 0.0901 - val_accuracy: 0.8582 - val_loss: 0.8609\n",
      "Epoch 231/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.9747 - loss: 0.0812 - val_accuracy: 0.8560 - val_loss: 0.8202\n",
      "Epoch 232/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.9745 - loss: 0.0742 - val_accuracy: 0.8525 - val_loss: 0.8619\n",
      "Epoch 233/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9729 - loss: 0.0868 - val_accuracy: 0.8496 - val_loss: 0.8936\n",
      "Epoch 234/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.9631 - loss: 0.1152 - val_accuracy: 0.8553 - val_loss: 0.8040\n",
      "Epoch 235/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9709 - loss: 0.0813 - val_accuracy: 0.8560 - val_loss: 0.8275\n",
      "Epoch 236/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9727 - loss: 0.0767 - val_accuracy: 0.8567 - val_loss: 0.8370\n",
      "Epoch 237/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9713 - loss: 0.0791 - val_accuracy: 0.8489 - val_loss: 0.8179\n",
      "Epoch 238/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9750 - loss: 0.0792 - val_accuracy: 0.8489 - val_loss: 0.8136\n",
      "Epoch 239/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9706 - loss: 0.0876 - val_accuracy: 0.8582 - val_loss: 0.8338\n",
      "Epoch 240/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9690 - loss: 0.0954 - val_accuracy: 0.8517 - val_loss: 0.7905\n",
      "Epoch 241/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.9697 - loss: 0.0865 - val_accuracy: 0.8553 - val_loss: 0.8484\n",
      "Epoch 242/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9736 - loss: 0.0772 - val_accuracy: 0.8546 - val_loss: 0.9036\n",
      "Epoch 243/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9734 - loss: 0.0835 - val_accuracy: 0.8460 - val_loss: 0.8679\n",
      "Epoch 244/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9713 - loss: 0.0849 - val_accuracy: 0.8489 - val_loss: 0.8544\n",
      "Epoch 245/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9693 - loss: 0.0973 - val_accuracy: 0.8596 - val_loss: 0.8985\n",
      "Epoch 246/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9690 - loss: 0.0996 - val_accuracy: 0.8603 - val_loss: 0.7495\n",
      "Epoch 247/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9722 - loss: 0.0919 - val_accuracy: 0.8560 - val_loss: 0.8977\n",
      "Epoch 248/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9740 - loss: 0.0669 - val_accuracy: 0.8453 - val_loss: 0.8916\n",
      "Epoch 249/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9663 - loss: 0.1123 - val_accuracy: 0.8532 - val_loss: 0.8876\n",
      "Epoch 250/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9743 - loss: 0.0818 - val_accuracy: 0.8503 - val_loss: 0.9067\n",
      "Epoch 251/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9752 - loss: 0.0730 - val_accuracy: 0.8603 - val_loss: 0.8665\n",
      "Epoch 252/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9743 - loss: 0.0786 - val_accuracy: 0.8475 - val_loss: 0.8192\n",
      "Epoch 253/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9747 - loss: 0.0791 - val_accuracy: 0.8596 - val_loss: 0.8667\n",
      "Epoch 254/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9766 - loss: 0.0673 - val_accuracy: 0.8574 - val_loss: 0.9070\n",
      "Epoch 255/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9736 - loss: 0.0776 - val_accuracy: 0.8553 - val_loss: 0.8729\n",
      "Epoch 256/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9741 - loss: 0.0744 - val_accuracy: 0.8589 - val_loss: 0.8346\n",
      "Epoch 257/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9720 - loss: 0.0791 - val_accuracy: 0.8460 - val_loss: 0.8713\n",
      "Epoch 258/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9709 - loss: 0.0814 - val_accuracy: 0.8389 - val_loss: 0.8276\n",
      "Epoch 259/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9670 - loss: 0.0993 - val_accuracy: 0.8432 - val_loss: 0.8725\n",
      "Epoch 260/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9706 - loss: 0.0819 - val_accuracy: 0.8510 - val_loss: 0.8737\n",
      "Epoch 261/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9745 - loss: 0.0718 - val_accuracy: 0.8503 - val_loss: 0.8817\n",
      "Epoch 262/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9774 - loss: 0.0653 - val_accuracy: 0.8546 - val_loss: 0.9044\n",
      "Epoch 263/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9779 - loss: 0.0717 - val_accuracy: 0.8610 - val_loss: 0.8399\n",
      "Epoch 264/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9674 - loss: 0.0921 - val_accuracy: 0.8503 - val_loss: 0.9557\n",
      "Epoch 265/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9688 - loss: 0.0925 - val_accuracy: 0.8560 - val_loss: 0.8717\n",
      "Epoch 266/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9774 - loss: 0.0774 - val_accuracy: 0.8460 - val_loss: 0.9257\n",
      "Epoch 267/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9734 - loss: 0.0820 - val_accuracy: 0.8667 - val_loss: 0.8377\n",
      "Epoch 268/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9772 - loss: 0.0629 - val_accuracy: 0.8624 - val_loss: 0.8811\n",
      "Epoch 269/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9752 - loss: 0.0714 - val_accuracy: 0.8617 - val_loss: 0.8893\n",
      "Epoch 270/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9788 - loss: 0.0673 - val_accuracy: 0.8539 - val_loss: 0.8661\n",
      "Epoch 271/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9740 - loss: 0.0779 - val_accuracy: 0.8546 - val_loss: 0.8544\n",
      "Epoch 272/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9736 - loss: 0.0798 - val_accuracy: 0.8596 - val_loss: 0.8605\n",
      "Epoch 273/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9779 - loss: 0.0655 - val_accuracy: 0.8610 - val_loss: 0.8970\n",
      "Epoch 274/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9731 - loss: 0.0761 - val_accuracy: 0.8532 - val_loss: 0.8353\n",
      "Epoch 275/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9749 - loss: 0.0695 - val_accuracy: 0.8653 - val_loss: 0.9211\n",
      "Epoch 276/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9727 - loss: 0.0841 - val_accuracy: 0.8596 - val_loss: 0.8739\n",
      "Epoch 277/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9715 - loss: 0.0968 - val_accuracy: 0.8539 - val_loss: 0.8483\n",
      "Epoch 278/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9766 - loss: 0.0702 - val_accuracy: 0.8525 - val_loss: 0.8399\n",
      "Epoch 279/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9795 - loss: 0.0677 - val_accuracy: 0.8553 - val_loss: 0.9301\n",
      "Epoch 280/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9775 - loss: 0.0633 - val_accuracy: 0.8646 - val_loss: 0.9485\n",
      "Epoch 281/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9713 - loss: 0.0876 - val_accuracy: 0.8475 - val_loss: 1.0061\n",
      "Epoch 282/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9693 - loss: 0.0924 - val_accuracy: 0.8553 - val_loss: 0.8524\n",
      "Epoch 283/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.9700 - loss: 0.1008 - val_accuracy: 0.8539 - val_loss: 0.8789\n",
      "Epoch 284/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.9708 - loss: 0.0912 - val_accuracy: 0.8596 - val_loss: 0.8945\n",
      "Epoch 285/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.9775 - loss: 0.0677 - val_accuracy: 0.8610 - val_loss: 0.8826\n",
      "Epoch 286/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9756 - loss: 0.0759 - val_accuracy: 0.8567 - val_loss: 0.8566\n",
      "Epoch 287/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.9702 - loss: 0.0870 - val_accuracy: 0.8546 - val_loss: 0.9148\n",
      "Epoch 288/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9766 - loss: 0.0651 - val_accuracy: 0.8646 - val_loss: 0.9060\n",
      "Epoch 289/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9813 - loss: 0.0597 - val_accuracy: 0.8553 - val_loss: 0.9274\n",
      "Epoch 290/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9784 - loss: 0.0719 - val_accuracy: 0.8553 - val_loss: 0.9191\n",
      "Epoch 291/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9770 - loss: 0.0693 - val_accuracy: 0.8589 - val_loss: 0.9547\n",
      "Epoch 292/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9740 - loss: 0.0789 - val_accuracy: 0.8503 - val_loss: 0.8929\n",
      "Epoch 293/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9718 - loss: 0.0817 - val_accuracy: 0.8482 - val_loss: 0.9592\n",
      "Epoch 294/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9747 - loss: 0.0715 - val_accuracy: 0.8525 - val_loss: 0.9471\n",
      "Epoch 295/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9743 - loss: 0.0801 - val_accuracy: 0.8496 - val_loss: 0.9176\n",
      "Epoch 296/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9765 - loss: 0.0683 - val_accuracy: 0.8574 - val_loss: 0.9284\n",
      "Epoch 297/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9756 - loss: 0.0651 - val_accuracy: 0.8553 - val_loss: 0.9061\n",
      "Epoch 298/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9799 - loss: 0.0602 - val_accuracy: 0.8510 - val_loss: 0.9329\n",
      "Epoch 299/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9806 - loss: 0.0598 - val_accuracy: 0.8475 - val_loss: 0.9220\n",
      "Epoch 300/300\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9757 - loss: 0.0778 - val_accuracy: 0.8517 - val_loss: 0.8908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=300, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39108b7c",
   "metadata": {},
   "source": [
    " เซฟโมเดลและ label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46145e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and labels to models/\n"
     ]
    }
   ],
   "source": [
    "import os, joblib\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model.save(\"models/chars74k_model.h5\")\n",
    "# เซฟ list ของ class เช่น ['0','1','2',...,'A',...]\n",
    "joblib.dump(le.classes_.tolist(), \"models/chars74k_labels.pkl\")\n",
    "print(\"Saved model and labels to models/\")\n",
    "# บันทึกโมเดลที่ฝึกแล้วและ mapping ของ labels\n",
    "# - สร้างโฟลเดอร์ models/ ถ้ายังไม่มี\n",
    "# - เซฟไฟล์โมเดล .h5 และไฟล์ labels (list ของคลาสตามลำดับ output ของ softmax)\n",
    "import os, joblib\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model.save(\"models/chars74k_model.h5\")\n",
    "# เซฟ list ของ class เช่น ['0','1','2',...,'A',...] (ใช้เมื่อนำโมเดลไปใช้งาน)\n",
    "joblib.dump(le.classes_.tolist(), \"models/chars74k_labels.pkl\")\n",
    "print(\"Saved model and labels to models/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
